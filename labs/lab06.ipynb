{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your name and collaborators below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"Guo Fangzheng\"\n",
    "COLLABORATORS = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1aef1b70bb60105f88a2ecdb7c48db7c",
     "grade": false,
     "grade_id": "intro",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "# Lab 6: Multiple Linear Regression and Feature Engineering\n",
    "\n",
    "In this lab, we will work through the process of:\n",
    "1. Implementing a linear model \n",
    "1. Defining loss functions\n",
    "1. Feature engineering\n",
    "1. Minimizing loss functions using numeric libraries and analytical methods \n",
    "\n",
    "This lab will continue using the toy tip calculation dataset used in Lab 5.\n",
    "\n",
    "**This assignment should be completed and submitted by Monday October 1, 2018 at 11:59pm**\n",
    "\n",
    "### Collaboration Policy\n",
    "\n",
    "Data science is a collaborative activity. While you may talk with others about the labs, we ask that you **write your solutions individually**. If you do discuss the assignments with others, please **include their names** at the top of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "85ddf3c07784bf3b9d2dc8c9f20059f4",
     "grade": false,
     "grade_id": "imports",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(42)\n",
    "plt.style.use('fivethirtyeight')\n",
    "sns.set()\n",
    "sns.set_context(\"talk\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b3f005a87c853ee37609c2c01d573d17",
     "grade": false,
     "grade_id": "load-data-text",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "# Loading the Tips Dataset\n",
    "\n",
    "To begin, let's load the tips dataset from the `seaborn` library.  This dataset contains records of tips, total bill, and information about the person who paid the bill. This is the same dataset used in Lab 5, so it should look familiar!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1ace5e77d1a3d0c640ff5a43362ba815",
     "grade": false,
     "grade_id": "load-data-code",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Records: 244\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total_bill</th>\n",
       "      <th>tip</th>\n",
       "      <th>sex</th>\n",
       "      <th>smoker</th>\n",
       "      <th>day</th>\n",
       "      <th>time</th>\n",
       "      <th>size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16.99</td>\n",
       "      <td>1.01</td>\n",
       "      <td>Female</td>\n",
       "      <td>No</td>\n",
       "      <td>Sun</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.34</td>\n",
       "      <td>1.66</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>Sun</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21.01</td>\n",
       "      <td>3.50</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>Sun</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23.68</td>\n",
       "      <td>3.31</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>Sun</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24.59</td>\n",
       "      <td>3.61</td>\n",
       "      <td>Female</td>\n",
       "      <td>No</td>\n",
       "      <td>Sun</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   total_bill   tip     sex smoker  day    time  size\n",
       "0       16.99  1.01  Female     No  Sun  Dinner     2\n",
       "1       10.34  1.66    Male     No  Sun  Dinner     3\n",
       "2       21.01  3.50    Male     No  Sun  Dinner     3\n",
       "3       23.68  3.31    Male     No  Sun  Dinner     2\n",
       "4       24.59  3.61  Female     No  Sun  Dinner     4"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = sns.load_dataset(\"tips\")\n",
    "\n",
    "print(\"Number of Records:\", len(data))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "60335ee06f8b4d7161486c7cd46dc8ab",
     "grade": false,
     "grade_id": "q1",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "# Question 1: Defining the Model and Feature Engineering\n",
    "\n",
    "In the previous lab, we defined a simple linear model with only one parameter. Now let's make a more complicated model that utilizes other features in our dataset. Let our prediction for tip be a combination of the following features:\n",
    "\n",
    "$$\n",
    "\\text{Tip} = \\theta_1 \\cdot \\text{total_bill} + \\theta_2 \\cdot \\text{sex} + \\theta_3 \\cdot \\text{smoker} + \\theta_4 \\cdot \\text{day} + \\theta_5 \\cdot \\text{time} + \\theta_6 \\cdot \\text{size}\n",
    "$$\n",
    "\n",
    "Notice that some of these features are not numbers! But our linear model will need to predict a numerical value. Let's start by converting some of these non-numerical values into numerical values. Below we split the tips and the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5671bb8515dc1e2c32b269b503af2e88",
     "grade": false,
     "grade_id": "split-data",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "tips = data['tip']\n",
    "X = data.drop(columns='tip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6d267cfc7a8cf2f05457a669cd914565",
     "grade": false,
     "grade_id": "q1a",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "## Question 1a: Feature Engineering\n",
    "\n",
    "First, let's convert our features to numerical values. A straightforward approach is to map some of these non-numerical features into numerical ones. \n",
    "\n",
    "For example, we can treat the day as a value from 1-7. However, one of the disadvantages in directly translating to a numeric value is that we unintentionally assign certain features disproportionate weight. Consider assigning Sunday to the numeric value of 7, and Monday to the numeric value of 1. In our linear model, Sunday will have 7 times the influence of Monday, which can lower the accuracy of our model.\n",
    "\n",
    "Instead, let's use one-hot encoding to better represent these features! \n",
    "\n",
    "As discussed in lecture, one-hot encoding will produce a binary vector indicating the non-numeric feature. Sunday would be encoded as a [0 0 0 0 0 0 1]. This assigns a more even weight across each category in non-numeric features. Complete the code below to one-hot encode our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0890448a735ed28d6552f5603eb39098",
     "grade": false,
     "grade_id": "q1a-answer",
     "locked": false,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def one_hot_encode(data):\n",
    "    \"\"\"\n",
    "    Return the one-hot encoded dataframe of our input, data.\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    data: a dataframe that may include non-numerical features\n",
    "    \n",
    "    Returns\n",
    "    -----------\n",
    "    A one-hot encoded dataframe that only contains numeric features\n",
    "    \n",
    "    Hint: Check out the pd.get_dummies function\n",
    "    \"\"\"\n",
    "    a=pd.get_dummies(data)\n",
    "    return a\n",
    "    # YOUR CODE HERE\n",
    "   # raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# b=pd.concat([X,a],axis=1)\n",
    "# b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f47f93434ee74630a72ff1bb98ef7a5b",
     "grade": false,
     "grade_id": "q1a-result-instruction",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "Run the cell below to see the transformed dataset named `one_hot_X`. This dataframe holds our \"featurized\" data, which is also often denoted by $\\phi$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b12e1ad9db21f73a33d7b31c596cc410",
     "grade": false,
     "grade_id": "q1a-result",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total_bill</th>\n",
       "      <th>size</th>\n",
       "      <th>sex_Male</th>\n",
       "      <th>sex_Female</th>\n",
       "      <th>smoker_Yes</th>\n",
       "      <th>smoker_No</th>\n",
       "      <th>day_Thur</th>\n",
       "      <th>day_Fri</th>\n",
       "      <th>day_Sat</th>\n",
       "      <th>day_Sun</th>\n",
       "      <th>time_Lunch</th>\n",
       "      <th>time_Dinner</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16.99</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.34</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21.01</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23.68</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24.59</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   total_bill  size  sex_Male  sex_Female  smoker_Yes  smoker_No  day_Thur  \\\n",
       "0       16.99     2         0           1           0          1         0   \n",
       "1       10.34     3         1           0           0          1         0   \n",
       "2       21.01     3         1           0           0          1         0   \n",
       "3       23.68     2         1           0           0          1         0   \n",
       "4       24.59     4         0           1           0          1         0   \n",
       "\n",
       "   day_Fri  day_Sat  day_Sun  time_Lunch  time_Dinner  \n",
       "0        0        0        1           0            1  \n",
       "1        0        0        1           0            1  \n",
       "2        0        0        1           0            1  \n",
       "3        0        0        1           0            1  \n",
       "4        0        0        1           0            1  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_X = one_hot_encode(X)\n",
    "one_hot_X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f32a04280146d024ebe74d839b0f04fa",
     "grade": true,
     "grade_id": "q1a-tests",
     "locked": true,
     "points": 1,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert one_hot_X.shape == (244, 12)\n",
    "# Makes sure all columns are numeric\n",
    "assert np.all([np.issubdtype(one_hot_X[column].dtype, np.number) for column in one_hot_X])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "da53304642755590a282578193086122",
     "grade": false,
     "grade_id": "q1b",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "## Question 1b: Defining the Model\n",
    "\n",
    "Now that all of our data is numeric, we can begin to define our model function. Notice that after one-hot encoding our data, we now have 12 features instead of 6. Therefore, our linear model now looks like:\n",
    "\n",
    "$$\n",
    "\\text{Tip} = \\theta_1 \\cdot \\text{size} + \\theta_2 \\cdot \\text{total_bill} + \\theta_3 \\cdot \\text{day_Thur} + \\theta_4 \\cdot \\text{day_Fri} + ... + \\theta_{11} \\cdot \\text{time_Lunch} + \\theta_{12} \\cdot \\text{time_Dinner}\n",
    "$$\n",
    "\n",
    "We can represent the linear combination above as a matrix-vector product. Implement the `linear_model` function to evaluate this product.\n",
    "\n",
    "**Hint**: You can use [np.dot](https://docs.scipy.org/doc/numpy-1.15.0/reference/generated/numpy.dot.html), [pd.DataFrame.dot](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.dot.html), or the `@` operator to multiply matrices/vectors. However, while the `@` operator can be used to multiply `numpy` arrays, it generally will not work between two `pandas` objects, so keep that in mind when computing matrix-vector products!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f12a808a9b03f5a9e42dca168ceeae15",
     "grade": false,
     "grade_id": "q1b-answer",
     "locked": false,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def linear_model(thetas, X):\n",
    "    \"\"\"\n",
    "    Return the linear combination of thetas and features as defined above.\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    thetas: a 1D vector representing the parameters of our model ([theta1, theta2, ...])\n",
    "    X: a 2D dataframe of numeric features\n",
    "    \n",
    "    Returns\n",
    "    -----------\n",
    "    A 1D vector representing the linear combination of thetas and features as defined above.\n",
    "    \"\"\"\n",
    "    a=np.dot(X,thetas)\n",
    "    return a\n",
    "    # YOUR CODE HERE\n",
    "    #raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0dd92321dda42afdf0897701b9d6ff05",
     "grade": true,
     "grade_id": "q1b-tests",
     "locked": true,
     "points": 1,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert linear_model(np.arange(1,5), np.arange(1,5)) == 30\n",
    "assert (linear_model(2*np.eye(100), np.ones(100)) == 2*np.ones(100)).all()\n",
    "test_theta = np.array([[1, 2], [3, 4], [5, 6]])\n",
    "test_x = np.array([[1, 3, 5], [2, 4, 6]])\n",
    "expected = np.array([[35, 44], [44, 56]])\n",
    "actual = linear_model(test_theta, test_x)\n",
    "assert np.array_equal(actual, expected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f320def4cee4f55f929b015b20489941",
     "grade": false,
     "grade_id": "q2",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "# Question 2: Fitting the Model using Numeric Methods\n",
    "\n",
    "Recall in the previous lab we defined multiple loss functions and found optimal theta using the scipy.minimize function. Adapt the loss functions and optimization code from the previous lab (provided below) to work with our new linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8a1d014844df5974d2a3a1cfd15ae802",
     "grade": false,
     "grade_id": "q2-answer",
     "locked": false,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.094487  ,  0.1759912 ,  0.18411085,  0.21655235,  0.15712761,\n",
       "        0.24353555,  0.01519972,  0.17746092,  0.05600946,  0.15199343,\n",
       "        0.23440129,  0.1662616 ])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "def l1(y, y_hat):\n",
    "    return np.abs(y - y_hat)\n",
    "\n",
    "def l2(y, y_hat):\n",
    "    return (y - y_hat)**2\n",
    "\n",
    "def minimize_average_loss(loss_function, model, X, y):\n",
    "    \"\"\"\n",
    "    Minimize the average loss calculated from using different theta vectors, and \n",
    "    estimate the optimal theta for the model.\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    loss_function: either the squared or absolute loss functions defined above\n",
    "    model: the model (as defined in Question 1b)\n",
    "    X: a 2D dataframe of numeric features (one-hot encoded)\n",
    "    y: a 1D vector of tip amounts\n",
    "    \n",
    "    Returns\n",
    "    -----------\n",
    "    The estimate for the optimal theta vector that minimizes our loss\n",
    "    \"\"\"\n",
    "    \n",
    "    ## Notes on the following function call which you need to finish:\n",
    "    # \n",
    "    # 0. The first '...' should be replaced with the average loss evaluated on \n",
    "    #       the data X, y using the model and appropriate loss function.\n",
    "    # 1. x0 are the initial values for THETA.  Yes, this is confusing\n",
    "    #       but optimization people like x to be the thing they are \n",
    "    #       optimizing. Replace the second '...' with an initial value for theta,\n",
    "    #       and remember that theta is now a vector. DO NOT hard-code the length of x0;\n",
    "    #       it should depend on the number of features in X.\n",
    "    # 2. Your answer will be very similar to your answer to question 2 from lab 5.\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    #return minimize(lambda theta: , x0=...)['x']\n",
    "    return minimize(lambda theta:np.mean(loss_function(model(theta,X),y)), x0=np.zeros(X.shape[1]))['x'] # We extract 'x' entry in dict, which contains optimal theta\n",
    "    #raise NotImplementedError()\n",
    "    # Notice above that we extract the 'x' entry in the dictionary returned by `minimize`. \n",
    "    # This entry corresponds to the optimal theta estimated by the function.\n",
    "\n",
    "minimize_average_loss(l2, linear_model, one_hot_X, tips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8156710233a3bce66c3c29b53cb95e4f",
     "grade": true,
     "grade_id": "q2-tests",
     "locked": true,
     "points": 1,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "expected_l1 = np.array([0.1021857, 0.08590592, 0.1181548, 0.28102618, 0.13510419, 0.2640767, \n",
    "                        -0.0567563, 0.20579663, -0.03585098, 0.28599161, 0.17696859, 0.22221226])\n",
    "expected_l2 = np.array([0.094487, 0.1759912, 0.18411085, 0.21655235, 0.15712761, 0.24353555, \n",
    "                        0.01519972, 0.17746092, 0.05600946, 0.15199343, 0.23440129, 0.1662616])\n",
    "actual_l1 = np.array(minimize_average_loss(l1, linear_model, one_hot_X, tips))\n",
    "actual_l2 = np.array(minimize_average_loss(l2, linear_model, one_hot_X, tips))\n",
    "\n",
    "# Check if your values for theta predict similar tips as our values for theta do\n",
    "assert np.isclose(one_hot_X @ expected_l1, one_hot_X @ actual_l1, rtol=0.1).all()\n",
    "assert np.isclose(one_hot_X @ expected_l2, one_hot_X @ actual_l2, rtol=0.001).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7a9db2a3da2abb04a922e135e34aba06",
     "grade": false,
     "grade_id": "q3",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "# Question 3: Fitting the Model using Analytic Methods\n",
    "\n",
    "Let's also fit our model analytically, for the l2 loss function. In this question we will derive an analytical solution, fit our model and compare our results with our numerical optimization results.\n",
    "\n",
    "Recall that if we're fitting a linear model with the l2 loss function, we are performing least squares! Remember, we are solving the following optimization problem for least squares:\n",
    "\n",
    "$$\\min_{\\theta} ||X\\theta - y||^2$$\n",
    "\n",
    "Let's begin by deriving the analytic solution to least squares. We begin by expanding out the l2 norm and multiplying out all of the terms.\n",
    "\n",
    "<table style=\"width:75%\">\n",
    "  <tr>\n",
    "    <th style=\"text-align: center\">Math</th>\n",
    "    <th style=\"text-align: center\">Explanation</th> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>$$||X\\theta - y||^2 = (X\\theta - y)^T (X\\theta - y)$$</td>\n",
    "    <td>Expand l2 norm using the definition for matrices.</td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>$$ = (\\theta^T X^T - y^T) (X\\theta - y)$$</td>\n",
    "    <td>Distribute the transpose operator. Remember that $(AB)^T = B^TA^T$.</td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>$$ = \\theta^T X^T X \\theta - \\theta^T X^T y - y^T X \\theta + y^T y$$</td>\n",
    "    <td>Multiply out all of the terms (FOIL).</td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>$$ = \\theta^T X^T X \\theta - 2\\theta^T X^T y + y^T y$$</td>\n",
    "    <td>The two middle terms are both transposes of each other, and they are both scalars (since we have a 1xn row vector times an nxn matrix times an nx1 column vector). Since the transpose of a scalar is still the same scalar, we can combine the two middle terms.</td> \n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "Whew! Now that we have everything expanded out and somewhat simplified, let's take the gradient of the expression above and set it to the zero vector. This will allow us to solve for the optimal $\\theta$ that will minimize our loss.\n",
    "\n",
    "<table style=\"width:75%\">\n",
    "  <tr>\n",
    "    <th style=\"text-align: center\">Math</th>\n",
    "    <th style=\"text-align: center\">Explanation</th> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>$$\\nabla_\\theta (\\theta^T X^T X \\theta) - \\nabla_\\theta(2\\theta^TX^T y) + \\nabla_\\theta(y^T y) = \\vec{0}$$</td>\n",
    "    <td>Let's take derivatives one term at a time.</td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>$$(X^T X + (X^T X)^T)\\theta - \\nabla_\\theta(2\\theta^TX^T y) + \\nabla_\\theta(y^T y) = \\vec{0}$$</td>\n",
    "    <td>For the first term, we use the identity $\\frac{\\partial}{\\partial x} x^T A x = (A + A^T)x$.</td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>$$(X^T X + (X^T X)^T)\\theta - 2X^T y + \\nabla_\\theta(y^T y) = \\vec{0}$$</td>\n",
    "    <td>For the second term, we use the identity $\\frac{\\partial}{\\partial x} x^T A = A$.</td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>$$(X^T X + (X^T X)^T)\\theta - 2X^T y + \\vec{0} = \\vec{0}$$</td>\n",
    "    <td>The last derivative is the easiest, since $y^T y$ does not depend on $\\theta$.</td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>$$2X^T X\\theta = 2X^T y$$</td>\n",
    "    <td>Notice that $(X^T X)^T = X^T X$, so we can combine the $X^T X$ terms into $2X^TX$. We also move $2X^Ty$ to the right side of the equation.</td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>$$\\theta = (X^T X)^{-1} X^T y$$</td>\n",
    "    <td>Divide by 2 on both sides, then left-multiply by $(X^T X)^{-1}$ on both sides to solve for $\\theta$.</td> \n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f466a235cad9346f34c7da25e27c5a58",
     "grade": false,
     "grade_id": "q3a",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "## Question 3a: Solving for Theta\n",
    "\n",
    "Now that we have the analytic solution for $\\theta$, let's find the optimal numerical thetas for our tips dataset. Fill out the function below. \n",
    "\n",
    "Hints:\n",
    "1. Use the `np.linalg.inv` function to compute matrix inverses\n",
    "1. To compute the transpose of a matrix, you can use `X.T` or `X.transpose()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[  1.01],\n",
       "        [  1.66],\n",
       "        [  3.5 ],\n",
       "        [  3.31],\n",
       "        [  3.61],\n",
       "        [  4.71],\n",
       "        [  2.  ],\n",
       "        [  3.12],\n",
       "        [  1.96],\n",
       "        [  3.23],\n",
       "        [  1.71],\n",
       "        [  5.  ],\n",
       "        [  1.57],\n",
       "        [  3.  ],\n",
       "        [  3.02],\n",
       "        [  3.92],\n",
       "        [  1.67],\n",
       "        [  3.71],\n",
       "        [  3.5 ],\n",
       "        [  3.35],\n",
       "        [  4.08],\n",
       "        [  2.75],\n",
       "        [  2.23],\n",
       "        [  7.58],\n",
       "        [  3.18],\n",
       "        [  2.34],\n",
       "        [  2.  ],\n",
       "        [  2.  ],\n",
       "        [  4.3 ],\n",
       "        [  3.  ],\n",
       "        [  1.45],\n",
       "        [  2.5 ],\n",
       "        [  3.  ],\n",
       "        [  2.45],\n",
       "        [  3.27],\n",
       "        [  3.6 ],\n",
       "        [  2.  ],\n",
       "        [  3.07],\n",
       "        [  2.31],\n",
       "        [  5.  ],\n",
       "        [  2.24],\n",
       "        [  2.54],\n",
       "        [  3.06],\n",
       "        [  1.32],\n",
       "        [  5.6 ],\n",
       "        [  3.  ],\n",
       "        [  5.  ],\n",
       "        [  6.  ],\n",
       "        [  2.05],\n",
       "        [  3.  ],\n",
       "        [  2.5 ],\n",
       "        [  2.6 ],\n",
       "        [  5.2 ],\n",
       "        [  1.56],\n",
       "        [  4.34],\n",
       "        [  3.51],\n",
       "        [  3.  ],\n",
       "        [  1.5 ],\n",
       "        [  1.76],\n",
       "        [  6.73],\n",
       "        [  3.21],\n",
       "        [  2.  ],\n",
       "        [  1.98],\n",
       "        [  3.76],\n",
       "        [  2.64],\n",
       "        [  3.15],\n",
       "        [  2.47],\n",
       "        [  1.  ],\n",
       "        [  2.01],\n",
       "        [  2.09],\n",
       "        [  1.97],\n",
       "        [  3.  ],\n",
       "        [  3.14],\n",
       "        [  5.  ],\n",
       "        [  2.2 ],\n",
       "        [  1.25],\n",
       "        [  3.08],\n",
       "        [  4.  ],\n",
       "        [  3.  ],\n",
       "        [  2.71],\n",
       "        [  3.  ],\n",
       "        [  3.4 ],\n",
       "        [  1.83],\n",
       "        [  5.  ],\n",
       "        [  2.03],\n",
       "        [  5.17],\n",
       "        [  2.  ],\n",
       "        [  4.  ],\n",
       "        [  5.85],\n",
       "        [  3.  ],\n",
       "        [  3.  ],\n",
       "        [  3.5 ],\n",
       "        [  1.  ],\n",
       "        [  4.3 ],\n",
       "        [  3.25],\n",
       "        [  4.73],\n",
       "        [  4.  ],\n",
       "        [  1.5 ],\n",
       "        [  3.  ],\n",
       "        [  1.5 ],\n",
       "        [  2.5 ],\n",
       "        [  3.  ],\n",
       "        [  2.5 ],\n",
       "        [  3.48],\n",
       "        [  4.08],\n",
       "        [  1.64],\n",
       "        [  4.06],\n",
       "        [  4.29],\n",
       "        [  3.76],\n",
       "        [  4.  ],\n",
       "        [  3.  ],\n",
       "        [  1.  ],\n",
       "        [  4.  ],\n",
       "        [  2.55],\n",
       "        [  4.  ],\n",
       "        [  3.5 ],\n",
       "        [  5.07],\n",
       "        [  1.5 ],\n",
       "        [  1.8 ],\n",
       "        [  2.92],\n",
       "        [  2.31],\n",
       "        [  1.68],\n",
       "        [  2.5 ],\n",
       "        [  2.  ],\n",
       "        [  2.52],\n",
       "        [  4.2 ],\n",
       "        [  1.48],\n",
       "        [  2.  ],\n",
       "        [  2.  ],\n",
       "        [  2.18],\n",
       "        [  1.5 ],\n",
       "        [  2.83],\n",
       "        [  1.5 ],\n",
       "        [  2.  ],\n",
       "        [  3.25],\n",
       "        [  1.25],\n",
       "        [  2.  ],\n",
       "        [  2.  ],\n",
       "        [  2.  ],\n",
       "        [  2.75],\n",
       "        [  3.5 ],\n",
       "        [  6.7 ],\n",
       "        [  5.  ],\n",
       "        [  5.  ],\n",
       "        [  2.3 ],\n",
       "        [  1.5 ],\n",
       "        [  1.36],\n",
       "        [  1.63],\n",
       "        [  1.73],\n",
       "        [  2.  ],\n",
       "        [  2.5 ],\n",
       "        [  2.  ],\n",
       "        [  2.74],\n",
       "        [  2.  ],\n",
       "        [  2.  ],\n",
       "        [  5.14],\n",
       "        [  5.  ],\n",
       "        [  3.75],\n",
       "        [  2.61],\n",
       "        [  2.  ],\n",
       "        [  3.5 ],\n",
       "        [  2.5 ],\n",
       "        [  2.  ],\n",
       "        [  2.  ],\n",
       "        [  3.  ],\n",
       "        [  3.48],\n",
       "        [  2.24],\n",
       "        [  4.5 ],\n",
       "        [  1.61],\n",
       "        [  2.  ],\n",
       "        [ 10.  ],\n",
       "        [  3.16],\n",
       "        [  5.15],\n",
       "        [  3.18],\n",
       "        [  4.  ],\n",
       "        [  3.11],\n",
       "        [  2.  ],\n",
       "        [  2.  ],\n",
       "        [  4.  ],\n",
       "        [  3.55],\n",
       "        [  3.68],\n",
       "        [  5.65],\n",
       "        [  3.5 ],\n",
       "        [  6.5 ],\n",
       "        [  3.  ],\n",
       "        [  5.  ],\n",
       "        [  3.5 ],\n",
       "        [  2.  ],\n",
       "        [  3.5 ],\n",
       "        [  4.  ],\n",
       "        [  1.5 ],\n",
       "        [  4.19],\n",
       "        [  2.56],\n",
       "        [  2.02],\n",
       "        [  4.  ],\n",
       "        [  1.44],\n",
       "        [  2.  ],\n",
       "        [  5.  ],\n",
       "        [  2.  ],\n",
       "        [  2.  ],\n",
       "        [  4.  ],\n",
       "        [  2.01],\n",
       "        [  2.  ],\n",
       "        [  2.5 ],\n",
       "        [  4.  ],\n",
       "        [  3.23],\n",
       "        [  3.41],\n",
       "        [  3.  ],\n",
       "        [  2.03],\n",
       "        [  2.23],\n",
       "        [  2.  ],\n",
       "        [  5.16],\n",
       "        [  9.  ],\n",
       "        [  2.5 ],\n",
       "        [  6.5 ],\n",
       "        [  1.1 ],\n",
       "        [  3.  ],\n",
       "        [  1.5 ],\n",
       "        [  1.44],\n",
       "        [  3.09],\n",
       "        [  2.2 ],\n",
       "        [  3.48],\n",
       "        [  1.92],\n",
       "        [  3.  ],\n",
       "        [  1.58],\n",
       "        [  2.5 ],\n",
       "        [  2.  ],\n",
       "        [  3.  ],\n",
       "        [  2.72],\n",
       "        [  2.88],\n",
       "        [  2.  ],\n",
       "        [  3.  ],\n",
       "        [  3.39],\n",
       "        [  1.47],\n",
       "        [  3.  ],\n",
       "        [  1.25],\n",
       "        [  1.  ],\n",
       "        [  1.17],\n",
       "        [  4.67],\n",
       "        [  5.92],\n",
       "        [  2.  ],\n",
       "        [  2.  ],\n",
       "        [  1.75],\n",
       "        [  3.  ]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.matrix(tips).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "52ae35ad22b68617e4d565618f616586",
     "grade": false,
     "grade_id": "q3a-answer",
     "locked": false,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def get_analytical_sol(X, y):\n",
    "    \"\"\"\n",
    "    Computes the analytical solution to our least squares problem\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    X: a 2D dataframe of numeric features (one-hot encoded)\n",
    "    y: a 1D vector of tip amounts\n",
    "    \n",
    "    Returns\n",
    "    -----------\n",
    "    The estimate for theta computed using the equation mentioned above\n",
    "    \"\"\"\n",
    "    X=np.matrix(X)\n",
    "    y=np.matrix(y).T\n",
    "    a=np.dot(X.T,X)\n",
    "    b=np.linalg.inv(a)\n",
    "    c=np.dot(b,X.T)\n",
    "    theta=np.dot(c,y)\n",
    "#     d=np.dot(theta.T,X.T)\n",
    "#     e=np.dot(d,X)\n",
    "#     ff=np.dot(e,theta)\n",
    "#     gg=2*np.dot(d,y)\n",
    "#     kk=np.dot(y.T,y)\n",
    "    bbbb=[]\n",
    "    aaaa=theta.tolist()\n",
    "    for i in range(len(aaaa)):\n",
    "        bbbb.append(aaaa[i][0])\n",
    "    return bbbb\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4.050679240674155,\n",
       " -26.09957896590664,\n",
       " -2991737213141303.0,\n",
       " -2991737213141273.5,\n",
       " -1438173392860230.8,\n",
       " -1438173392860231.2,\n",
       " 4429910606001611.0,\n",
       " 4429910606001615.0,\n",
       " 4429910606001616.5,\n",
       " 4429910606001617.5,\n",
       " -69.80984375,\n",
       " -83.90906249999998]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analytical_thetas = get_analytical_sol(one_hot_X, tips)\n",
    "analytical_thetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3ae7d4aadc6464102d7c4ea70c900ec8",
     "grade": false,
     "grade_id": "q3a-results",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our analytical loss is:  1409.39696154\n",
      "Our numerical loss is:  1.01035356124\n"
     ]
    }
   ],
   "source": [
    "analytical_thetas = get_analytical_sol(one_hot_X, tips)\n",
    "print(\"Our analytical loss is: \", l2(linear_model(analytical_thetas, one_hot_X),tips).mean())\n",
    "print(\"Our numerical loss is: \", l2(linear_model(minimize_average_loss(l2, linear_model, one_hot_X, tips), one_hot_X), tips).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2eadd7d9f8a28057855f5e246227d512",
     "grade": true,
     "grade_id": "q3a-tests",
     "locked": true,
     "points": 1,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-6f91b296bccf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manalytical_thetas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mone_hot_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtips\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m328.61824697\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "assert np.isclose(l2(linear_model(analytical_thetas, one_hot_X),tips).mean(), 328.61824697, rtol=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c1c24dc6ce1a3f3f9fd001980459eecf",
     "grade": false,
     "grade_id": "q3b",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "## Question 3b: Weird Results?\n",
    "Our analytical loss is surprisingly much worse than our numerical loss. Why is this? \n",
    "\n",
    "Hint: https://stackoverflow.com/questions/31256252/why-does-numpy-linalg-solve-offer-more-precise-matrix-inversions-than-numpy-li"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a16a4423f3b83b0936d57f3023220a59",
     "grade": true,
     "grade_id": "q3b-answer",
     "locked": false,
     "points": 1,
     "schema_version": 2,
     "solution": true
    }
   },
   "source": [
    "That is because np.linalg.solve(A, b) does not compute the inverse of A. It calls one of the gesv LAPACK routines, which first factorizes A using LU decomposition, then solves for x using forward and backward substitution.\n",
    "\n",
    "np.linalg.inv uses the same method to compute the inverse of A by solving for A-1 in A·A-1 = I where I is the identity. The factorization step is exactly the same as above, but it takes more floating point operations to solve for A-1 (an n×n matrix) than for x (an n-long vector). Additionally, if you then wanted to obtain x via the identity A-1·b = x then the extra matrix multiplication would incur yet more floating point operations, and therefore slower performance and more numerical error.\n",
    "\n",
    "we need to obtain x directly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission\n",
    "\n",
    "You're done!\n",
    "\n",
    "Before submitting this assignment, ensure to:\n",
    "\n",
    "1. Restart the Kernel (in the menubar, select Kernel->Restart & Run All)\n",
    "2. Validate the notebook by clicking the \"Validate\" button\n",
    "\n",
    "Finally, make sure to **submit** the assignment via the Assignments tab in Datahub"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
